{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41066c",
   "metadata": {},
   "source": [
    "# **Archisha Sinha**\n",
    "## Domain: Natural Language Processings\n",
    "## Topic: Text Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c79407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip uninstall nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aab0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3b9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ece4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e500b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b60677",
   "metadata": {},
   "outputs": [],
   "source": [
    "para= \"“Yes! it is possible, although not useful,to use all the English language’s \\\n",
    "punctuationin two sentences. However, it would not be possible(unless you were all-knowing)\\\n",
    "to use:a full stop, a question mark,an exclamation mark…all together in the same sentence;it is impossible – isn’t it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d9a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Yes! it is possible, although not useful,to use all the English language’s punctuationin two sentences. However, it would not be possible(unless you were all-knowing)to use:a full stop, a question mark,an exclamation mark…all together in the same sentence;it is impossible – isn’t it?\n"
     ]
    }
   ],
   "source": [
    "print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8fc8f",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9938f",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2288dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“Yes!', 'it is possible, although not useful,to use all the English language’s punctuationin two sentences.', 'However, it would not be possible(unless you were all-knowing)to use:a full stop, a question mark,an exclamation mark…all together in the same sentence;it is impossible – isn’t it?']\n"
     ]
    }
   ],
   "source": [
    "sentence_tok= sent_tokenize(para)\n",
    "print(sentence_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bb3ea",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7bf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Yes', '!', 'it', 'is', 'possible', ',', 'although', 'not', 'useful', ',', 'to', 'use', 'all', 'the', 'English', 'language', '’', 's', 'punctuationin', 'two', 'sentences', '.', 'However', ',', 'it', 'would', 'not', 'be', 'possible', '(', 'unless', 'you', 'were', 'all-knowing', ')', 'to', 'use', ':', 'a', 'full', 'stop', ',', 'a', 'question', 'mark', ',', 'an', 'exclamation', 'mark…all', 'together', 'in', 'the', 'same', 'sentence', ';', 'it', 'is', 'impossible', '–', 'isn', '’', 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "word_tok= word_tokenize(para)\n",
    "print(word_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e26342",
   "metadata": {},
   "source": [
    "### Word Tokenization Using extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b8cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Yes', '!', 'it', 'is', 'possible', ',', 'although', 'not', 'useful', ',', 'to', 'use', 'all', 'the', 'English', 'language', '’', 's', 'punctuationin', 'two', 'sentences', '.', 'However', ',', 'it', 'would', 'not', 'be', 'possible', '(', 'unless', 'you', 'were', 'all-knowing', ')', 'to', 'use', ':', 'a', 'full', 'stop', ',', 'a', 'question', 'mark', ',', 'an', 'exclamation', 'mark…all', 'together', 'in', 'the', 'same', 'sentence', ';', 'it', 'is', 'impossible', '–', 'isn', '’', 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "words_token=[]\n",
    "for sent in sentence_tok:\n",
    "    words_token.extend(word_tokenize(sent))\n",
    "print(words_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fb787",
   "metadata": {},
   "source": [
    "### To count number of word in the given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ca12a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be4e81",
   "metadata": {},
   "source": [
    "### Word Tokenization Using append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abddd9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['“', 'Yes', '!'], ['it', 'is', 'possible', ',', 'although', 'not', 'useful', ',', 'to', 'use', 'all', 'the', 'English', 'language', '’', 's', 'punctuationin', 'two', 'sentences', '.'], ['However', ',', 'it', 'would', 'not', 'be', 'possible', '(', 'unless', 'you', 'were', 'all-knowing', ')', 'to', 'use', ':', 'a', 'full', 'stop', ',', 'a', 'question', 'mark', ',', 'an', 'exclamation', 'mark…all', 'together', 'in', 'the', 'same', 'sentence', ';', 'it', 'is', 'impossible', '–', 'isn', '’', 't', 'it', '?']]\n"
     ]
    }
   ],
   "source": [
    "words_token=[]\n",
    "for sent in sentence_tok:\n",
    "    words_token.append(word_tokenize(sent))\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa621ddd",
   "metadata": {},
   "source": [
    "### Tokenization without nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac8400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“Yes!', 'it', 'is', 'possible,', 'although', 'not', 'useful,to', 'use', 'all', 'the', 'English', 'language’s', 'punctuationin', 'two', 'sentences.', 'However,', 'it', 'would', 'not', 'be', 'possible(unless', 'you', 'were', 'all-knowing)to', 'use:a', 'full', 'stop,', 'a', 'question', 'mark,an', 'exclamation', 'mark…all', 'together', 'in', 'the', 'same', 'sentence;it', 'is', 'impossible', '–', 'isn’t', 'it?']\n"
     ]
    }
   ],
   "source": [
    "Text= para.split()\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d426d",
   "metadata": {},
   "source": [
    "# CHANGE CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e490494",
   "metadata": {},
   "source": [
    "### Converting Para to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad22c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“yes! it is possible, although not useful,to use all the english language’s punctuationin two sentences. however, it would not be possible(unless you were all-knowing)to use:a full stop, a question mark,an exclamation mark…all together in the same sentence;it is impossible – isn’t it?\n"
     ]
    }
   ],
   "source": [
    "para_lower= para.lower()\n",
    "print(para_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592cd94",
   "metadata": {},
   "source": [
    "### Converting Para to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bbcbcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“YES! IT IS POSSIBLE, ALTHOUGH NOT USEFUL,TO USE ALL THE ENGLISH LANGUAGE’S PUNCTUATIONIN TWO SENTENCES. HOWEVER, IT WOULD NOT BE POSSIBLE(UNLESS YOU WERE ALL-KNOWING)TO USE:A FULL STOP, A QUESTION MARK,AN EXCLAMATION MARK…ALL TOGETHER IN THE SAME SENTENCE;IT IS IMPOSSIBLE – ISN’T IT?\n"
     ]
    }
   ],
   "source": [
    "para_upper= para.upper()\n",
    "print(para_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2afa2",
   "metadata": {},
   "source": [
    "# STOP WORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a45dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0f119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfd740d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'both', 'for', 'off', 're', 'once', 'these', 'have', 'same', 'nor', \"won't\", 'into', 'few', \"you'll\", \"it's\", 'and', 'it', 'we', 'no', 'd', 'can', 'after', 'should', 'why', 've', 'myself', 'in', \"wasn't\", 'the', 'because', 'each', 'weren', 'does', 'who', 'more', 'such', 'did', 'were', 'was', 'doing', 'mustn', 'whom', 'down', 'to', \"don't\", 'further', 'y', 'haven', \"mustn't\", \"weren't\", \"you've\", 'itself', 'm', 'if', \"haven't\", 'she', 'between', 'them', 'what', 'be', 'hadn', 'yourself', 'most', 'i', 'out', \"wouldn't\", 'their', 'mightn', 'its', 'a', 'all', 'too', 'just', 'has', 'before', 'herself', 'will', 'or', 'as', 'll', 'needn', 'ain', 'an', 'ours', 'this', 'they', 'ourselves', 'how', 'you', 'other', 'that', 'above', \"aren't\", 'wouldn', 'below', 'during', 'themselves', \"shouldn't\", 'while', 'not', 'me', 'my', 'shan', 'ma', 'am', 'then', 't', 'there', 'wasn', \"she's\", 'but', 'your', 'he', 'won', \"shan't\", 'been', 'up', 'theirs', 'o', 'at', 'where', \"you'd\", \"that'll\", 'aren', 'isn', \"should've\", 'do', \"didn't\", 'here', 'his', \"isn't\", 'than', 'from', 'is', 'didn', 'now', 'her', 'which', 'about', 'having', 'under', 'some', \"hadn't\", 'being', 'him', 'through', 'against', 'until', 'when', 'yourselves', 'by', 'again', 'hers', \"doesn't\", \"you're\", 'with', 'himself', \"couldn't\", \"needn't\", 'shouldn', 'yours', 'any', 'own', 'very', 'over', 'so', 'are', 'our', 'had', \"mightn't\", 'couldn', 'doesn', 'of', 's', \"hasn't\", 'only', 'on', 'hasn', 'those', 'don'}\n"
     ]
    }
   ],
   "source": [
    "Stop_words= set(stopwords.words(\"english\"))\n",
    "print(Stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aabc27",
   "metadata": {},
   "source": [
    "### Para: LowerCase--> Word Tokenize--> Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88b0099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'yes', '!', 'possible', ',', 'although', 'useful', ',', 'use', 'english', 'language', '’', 'punctuationin', 'two', 'sentences', '.', 'however', ',', 'would', 'possible', '(', 'unless', 'all-knowing', ')', 'use', ':', 'full', 'stop', ',', 'question', 'mark', ',', 'exclamation', 'mark…all', 'together', 'sentence', ';', 'impossible', '–', '’', '?']\n"
     ]
    }
   ],
   "source": [
    "word_filtered=[]\n",
    "para_lower_tokenize=word_tokenize(para_lower)\n",
    "\n",
    "for word in para_lower_tokenize:\n",
    "    if word not in Stop_words:\n",
    "        word_filtered.append(word)\n",
    "print(word_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466f8a5",
   "metadata": {},
   "source": [
    "### Para: LowerCase--> Word Tokenize--> Removal of SPECIFIC Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ec8d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', '!', ',', 'although', 'not', 'useful', ',', 'use', 'all', 'the', 'english', 'language', '’', 's', 'punctuationin', 'two', 'sentences', '.', 'however', ',', 'would', 'not', 'be', '(', 'unless', 'you', 'were', 'all-knowing', ')', 'use', ':', 'a', 'full', 'stop', ',', 'a', 'question', 'mark', ',', 'an', 'exclamation', 'mark…all', 'together', 'the', 'same', 'sentence', ';', 'impossible', '–', 'isn', '’', 't', '?']\n"
     ]
    }
   ],
   "source": [
    "#Specific stop words to remove\n",
    "stop_words_list=[\"yes\", \"it\", \"is\", \"possible\", 'to', \"in\"]\n",
    "word_filtered=[]\n",
    "para_lower_tokenize=word_tokenize(para_lower)\n",
    "\n",
    "for word in para_lower_tokenize:\n",
    "    if word not in stop_words_list:\n",
    "        word_filtered.append(word)\n",
    "print(word_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f0359",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578f6d6",
   "metadata": {},
   "source": [
    "### Text processing task in which you reduce words to their root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6701bfa",
   "metadata": {},
   "source": [
    "#### PORTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6611c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e2cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  histori\n",
      "abitlity  :  abitl\n",
      "arrival  :  arriv\n",
      "finally  :  final\n",
      "congratulations  :  congratul\n",
      "exaggeration  :  exagger\n",
      "understandable  :  understand\n",
      "probability  :  probabl\n",
      "player  :  player\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fairli\n"
     ]
    }
   ],
   "source": [
    "words= [\"wait\", \"waiting\", \"waited\", \"waits\", \"listening\", \"history\", \"abitlity\", \"arrival\", \"finally\", \"congratulations\", \\\n",
    "       \"exaggeration\", \"understandable\", \"probability\", \"player\",\"toys\", \"consumer\", \"fairly\"]\n",
    "porter =PorterStemmer()\n",
    "for word in words:\n",
    "    rootWord=porter.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03228a27",
   "metadata": {},
   "source": [
    "#### LANCASTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dee6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ce65095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  list\n",
      "history  :  hist\n",
      "abitlity  :  abitl\n",
      "arrival  :  ar\n",
      "finally  :  fin\n",
      "congratulations  :  congrat\n",
      "exaggeration  :  exag\n",
      "understandable  :  understand\n",
      "probability  :  prob\n",
      "player  :  play\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "words= [\"wait\", \"waiting\", \"waited\", \"waits\", \"listening\", \"history\", \"abitlity\", \"arrival\", \"finally\", \"congratulations\", \\\n",
    "       \"exaggeration\", \"understandable\", \"probability\", \"player\",\"toys\", \"consumer\", \"fairly\"]\n",
    "lancaster =LancasterStemmer()\n",
    "for word in words:\n",
    "    rootWord=lancaster.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228e3d5",
   "metadata": {},
   "source": [
    "#### REGEXP STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8b060eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cafcfb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  waited\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  history\n",
      "abitlity  :  abitlity\n",
      "arrival  :  arrival\n",
      "finally  :  finally\n",
      "congratulations  :  congratulation\n",
      "exaggeration  :  exaggeration\n",
      "understandable  :  understand\n",
      "probability  :  probability\n",
      "player  :  play\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fairly\n"
     ]
    }
   ],
   "source": [
    "regex =RegexpStemmer(\"ing$|es$|s$|er$|able$\")\n",
    "\n",
    "for word in words:\n",
    "    rootWord=regex.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf0a32",
   "metadata": {},
   "source": [
    "#### SNOWBALL STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f37e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61acdcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  histori\n",
      "abitlity  :  abitl\n",
      "arrival  :  arriv\n",
      "finally  :  final\n",
      "congratulations  :  congratul\n",
      "exaggeration  :  exagger\n",
      "understandable  :  understand\n",
      "probability  :  probabl\n",
      "player  :  player\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "snow =SnowballStemmer(language=\"english\")\n",
    "\n",
    "for word in words:\n",
    "    rootWord=snow.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc5890",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544a8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Wordnet through NLTK in python console:\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cee9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait: wait\n",
      "waiting: waiting\n",
      "waited: waited\n",
      "waits: wait\n",
      "listening: listening\n",
      "history: history\n",
      "abitlity: abitlity\n",
      "arrival: arrival\n",
      "finally: finally\n",
      "congratulations: congratulation\n",
      "exaggeration: exaggeration\n",
      "understandable: understandable\n",
      "probability: probability\n",
      "player: player\n",
      "toys: toy\n",
      "consumer: consumer\n",
      "fairly: fairly\n"
     ]
    }
   ],
   "source": [
    "#Using nltk WordNet Lemmatizer\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "words= [\"wait\", \"waiting\", \"waited\", \"waits\", \"listening\", \"history\", \"abitlity\", \"arrival\", \"finally\", \"congratulations\", \\\n",
    "       \"exaggeration\", \"understandable\", \"probability\", \"player\",\"toys\", \"consumer\", \"fairly\"]\n",
    "for word in words:\n",
    "    print(\"{}: {}\".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2064c6",
   "metadata": {},
   "source": [
    "# PARTS OF SPEECH [POS] TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c356d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'students', 'are', 'waiting', 'outside', 'the', 'class', 'room']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room\"\n",
    "#Tokenizing the sentence\n",
    "word_tokens= word_tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d799a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'are', 'waiting', 'outside', 'the', 'class', 'room']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizing the tokens\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    lemmatized.append(lemmatizer.lemmatize(token))\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93c14106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student are waiting outside the class room\n"
     ]
    }
   ],
   "source": [
    "#Joining the lemmatized words\n",
    "lemmatized_sent= \" \".join(lemmatized)\n",
    "print(lemmatized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96bf8f",
   "metadata": {},
   "source": [
    "Note: We can observe that the words are not appropriately lemmatized. Eg, are does not become \"be\" or waiting does not become \"wait\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95e72e",
   "metadata": {},
   "source": [
    "### Using POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82ce16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"waiting\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"are\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8336f6",
   "metadata": {},
   "source": [
    "### Using nltk.pos_tag() method to tag words in large corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cffc14a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('waiting', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag([\"waiting\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75bf95ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('students', 'NNS'), ('are', 'VBP'), ('waiting', 'VBG'), ('outside', 'IN'), ('the', 'DT'), ('class', 'NN'), ('room', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "print(nltk.pos_tag(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb1b26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(word):\n",
    "    nltk_tag = nltk.pos_tag([word])[0][1][0]\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef059cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43421425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger(\"are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "783e1caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"waiting\",pos_tagger(\"waiting\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e84a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'be', 'wait', 'outside', 'the', 'class', 'room', 'eagerly']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room eagerly\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    tag= pos_tagger(token)\n",
    "    if tag==None:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    else:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token,tag))\n",
    "    \n",
    "    \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a90c47",
   "metadata": {},
   "source": [
    "Here we note that adverb eagerly is not getting lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a516c0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eagerly', 'RB')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"eagerly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "211c9617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger(\"eagerly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d212d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nicely'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"nicely\",\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f34c3",
   "metadata": {},
   "source": [
    "#### for adverb lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3ac4793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angry'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('angrily.r.1').lemmas()[0].pertainyms()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35cb2822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eager'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eagerly.r.1').lemmas()[0].pertainyms()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a8d6434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'be', 'wait', 'outside', 'the', 'class', 'room', 'eager']\n"
     ]
    }
   ],
   "source": [
    "#This code will lemmatize adverbs also\n",
    "\n",
    "sentence=\"The students are waiting outside the class room eagerly\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    tag= pos_tagger(token)\n",
    "    if tag==None:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    elif tag==\"r\":\n",
    "        lemmatized.append(wn.synset('{}.r.1'.format(token)).lemmas()[0].pertainyms()[0].name())\n",
    "        \n",
    "    else:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token,tag))\n",
    "    \n",
    "    \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83350c",
   "metadata": {},
   "source": [
    "# Find frequency of each word in a string in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "689453bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a241053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 5 samples and 7 outcomes>\n",
      "[('and', 2), ('practice', 2), ('Learn', 1), ('learn', 1), ('to', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Learn and practice and learn to practice\"\n",
    "words = text.split()\n",
    "fdist1 = FreqDist(words)\n",
    "print(fdist1)\n",
    "print(fdist1.most_common())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
