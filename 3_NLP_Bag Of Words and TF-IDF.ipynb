{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d91fff2",
   "metadata": {},
   "source": [
    "# **Archisha Sinha**\n",
    "## Domain: Natural Language Processings\n",
    "## Topic: Bag Of Words & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a9f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"India, country that occupies the greater part of South Asia. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4babe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=\"Its capital is New Delhi.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85028c60",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70ea9",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457d0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9c42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb442060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentence):\n",
    "    W_Tok= word_tokenize(sentence)\n",
    "    return(W_Tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc012ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', ',', 'country', 'that', 'occupies', 'the', 'greater', 'part', 'of', 'South', 'Asia', '.']\n",
      "['Its', 'capital', 'is', 'New', 'Delhi', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence1_WT= tokenization(sentence1)\n",
    "print(sentence1_WT)\n",
    "sentence2_WT= tokenization(sentence2)\n",
    "print(sentence2_WT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faaf31",
   "metadata": {},
   "source": [
    "# Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa78c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d7a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation(sentence):\n",
    "    \n",
    "    # Use NLTK's word_tokenize to tokenize the text into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Remove punctuation tokens\n",
    "    words_without_punct = [word for word in words if word not in string.punctuation]\n",
    "    \n",
    "    # Join the words back into a single string without punctuation\n",
    "    return(words_without_punct)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5ce857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'country', 'that', 'occupies', 'the', 'greater', 'part', 'of', 'South', 'Asia']\n",
      "['Its', 'capital', 'is', 'New', 'Delhi']\n"
     ]
    }
   ],
   "source": [
    "sentence1_WT_PR= punctuation(sentence1)\n",
    "print(sentence1_WT_PR)\n",
    "sentence2_WT_PR= punctuation(sentence2)\n",
    "print(sentence2_WT_PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8eff1c",
   "metadata": {},
   "source": [
    "#  Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30cb4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9c0ad",
   "metadata": {},
   "source": [
    "LIST OF STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f89afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'didn', 'further', 'hadn', 'there', 'where', 'isn', 'does', 'ours', 'other', \"should've\", 'should', \"you've\", 'ma', \"you'd\", 'her', 'few', 'again', 'no', 'its', 're', 'wasn', 'above', 'what', 'has', 'their', 'that', 'being', 've', 'as', \"wouldn't\", 'only', 'mightn', \"shouldn't\", 'itself', 'do', 'have', \"haven't\", 'in', \"mustn't\", 'during', 'themselves', 'haven', 'been', 'theirs', 'each', 'your', 'couldn', 'with', 'aren', 'is', 'up', 'just', \"didn't\", \"aren't\", 'all', \"doesn't\", 'my', 'had', 'it', 'was', 'yours', 'whom', 'y', 'him', 'more', 'to', 'nor', 'not', 'm', 'through', 'before', 'we', 'be', 'while', 'and', 'same', 'hers', 'o', \"it's\", 'mustn', 'you', 'too', 'once', 'under', 'now', 'own', 'below', 'how', \"that'll\", 'out', 'doing', \"couldn't\", 'this', 'yourself', 'those', 'having', \"you'll\", 'on', 'between', 'these', 'by', 'd', 'for', 'of', 'than', 'then', \"needn't\", 'into', 'when', 'off', \"weren't\", 'such', 'at', 'hasn', \"won't\", \"you're\", 'am', 'which', 'doesn', 'don', 'needn', \"mightn't\", 'wouldn', 'll', 'from', \"isn't\", \"wasn't\", 'did', 'i', 's', 't', 'if', 'won', 'so', 'our', \"don't\", 'or', 'after', 'over', 'ourselves', 'yourselves', 'here', 'will', 'myself', 'because', 'most', 'he', 'his', \"hadn't\", 'about', 'shouldn', 'me', 'down', \"hasn't\", 'both', 'herself', 'some', 'weren', 'why', 'himself', \"she's\", 'them', 'are', 'but', 'shan', \"shan't\", 'can', 'ain', 'a', 'until', 'she', 'an', 'were', 'who', 'any', 'they', 'very', 'the', 'against'}\n"
     ]
    }
   ],
   "source": [
    "Stop_words= set(stopwords.words(\"english\"))\n",
    "print(Stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b1408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop_words_list=[\"is\", \"the\", \"that\", \"of\", \"an\"]\n",
    "def stopwords(sentence):\n",
    "    sentence_SWR= []\n",
    "    for words in sentence:\n",
    "        if words not in Stop_words_list:\n",
    "            sentence_SWR.append(words)\n",
    "    return(sentence_SWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01f0d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'country', 'occupies', 'greater', 'part', 'South', 'Asia']\n",
      "['Its', 'capital', 'New', 'Delhi']\n"
     ]
    }
   ],
   "source": [
    "sentence1_WT_PR_SWR= stopwords(sentence1_WT_PR)\n",
    "print(sentence1_WT_PR_SWR)\n",
    "sentence2_WT_PR_SWR= stopwords(sentence2_WT_PR)\n",
    "print(sentence2_WT_PR_SWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a563ae",
   "metadata": {},
   "source": [
    "# Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251cf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(sentence):\n",
    "    sent=[]\n",
    "    for word in sentence:\n",
    "        sent.append(word.lower())\n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2e111f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'country', 'occupies', 'greater', 'part', 'south', 'asia']\n",
      "['its', 'capital', 'new', 'delhi']\n"
     ]
    }
   ],
   "source": [
    "sentence1_WT_PR_SWR_LC= lowercase(sentence1_WT_PR_SWR)\n",
    "print(sentence1_WT_PR_SWR_LC)\n",
    "sentence2_WT_PR_SWR_LC= lowercase(sentence2_WT_PR_SWR)\n",
    "print(sentence2_WT_PR_SWR_LC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238cc71",
   "metadata": {},
   "source": [
    "# Making a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c72d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'country', 'occupies', 'greater', 'part', 'south', 'asia', 'its', 'capital', 'new', 'delhi']\n"
     ]
    }
   ],
   "source": [
    "corpus= sentence1_WT_PR_SWR_LC + sentence2_WT_PR_SWR_LC\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63313bd4",
   "metadata": {},
   "source": [
    "# Sorting in Alphabetical Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4485f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia', 'capital', 'country', 'delhi', 'greater', 'india', 'its', 'new', 'occupies', 'part', 'south']\n"
     ]
    }
   ],
   "source": [
    "corpus.sort(reverse=False)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615013a",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db11511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b18ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex =RegexpStemmer(\"ing$|es$|s$|er$|able$\")\n",
    "def stemming(sentence):\n",
    "    for word in sentence:\n",
    "        rootWord=regex.stem(word)\n",
    "        print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7602569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india  :  india\n",
      "country  :  country\n",
      "occupies  :  occupi\n",
      "greater  :  great\n",
      "part  :  part\n",
      "south  :  south\n",
      "asia  :  asia\n",
      "None\n",
      "its  :  it\n",
      "capital  :  capital\n",
      "new  :  new\n",
      "delhi  :  delhi\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sentence1_WT_PR_SWR_LC_Stem= stemming(sentence1_WT_PR_SWR_LC)\n",
    "print(sentence1_WT_PR_SWR_LC_Stem)\n",
    "sentence2_WT_PR_SWR_LC_Stem= stemming(sentence2_WT_PR_SWR_LC)\n",
    "print(sentence2_WT_PR_SWR_LC_Stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea0e82",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0faa2",
   "metadata": {},
   "source": [
    "# LABLE ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3b9338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92c9dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a465b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(sentence):\n",
    "    encoded_labels = label_encoder.fit_transform(sentence)\n",
    "    return(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a4584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "Label_Encoded= label_encoding(corpus)\n",
    "print(Label_Encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d05821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asia': 0, 'capital': 1, 'country': 2, 'delhi': 3, 'greater': 4, 'india': 5, 'its': 6, 'new': 7, 'occupies': 8, 'part': 9, 'south': 10}\n"
     ]
    }
   ],
   "source": [
    "corpus_LE = {corpus[i]: Label_Encoded[i] for i in range(len(corpus))}\n",
    "\n",
    "print(corpus_LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161e0c9",
   "metadata": {},
   "source": [
    "# ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cadc63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be3a7b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels:\n",
      "      labels\n",
      "0       asia\n",
      "1    capital\n",
      "2    country\n",
      "3      delhi\n",
      "4    greater\n",
      "5      india\n",
      "6        its\n",
      "7        new\n",
      "8   occupies\n",
      "9       part\n",
      "10     south\n",
      "\n",
      "One-Hot Encoded:\n",
      "    labels_asia  labels_capital  labels_country  labels_delhi  labels_greater  \\\n",
      "0             1               0               0             0               0   \n",
      "1             0               1               0             0               0   \n",
      "2             0               0               1             0               0   \n",
      "3             0               0               0             1               0   \n",
      "4             0               0               0             0               1   \n",
      "5             0               0               0             0               0   \n",
      "6             0               0               0             0               0   \n",
      "7             0               0               0             0               0   \n",
      "8             0               0               0             0               0   \n",
      "9             0               0               0             0               0   \n",
      "10            0               0               0             0               0   \n",
      "\n",
      "    labels_india  labels_its  labels_new  labels_occupies  labels_part  \\\n",
      "0              0           0           0                0            0   \n",
      "1              0           0           0                0            0   \n",
      "2              0           0           0                0            0   \n",
      "3              0           0           0                0            0   \n",
      "4              0           0           0                0            0   \n",
      "5              1           0           0                0            0   \n",
      "6              0           1           0                0            0   \n",
      "7              0           0           1                0            0   \n",
      "8              0           0           0                1            0   \n",
      "9              0           0           0                0            1   \n",
      "10             0           0           0                0            0   \n",
      "\n",
      "    labels_south  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "5              0  \n",
      "6              0  \n",
      "7              0  \n",
      "8              0  \n",
      "9              0  \n",
      "10             1  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the original labels\n",
    "df = pd.DataFrame({'labels': corpus})\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df, columns=['labels'])\n",
    "\n",
    "print(\"Original Labels:\")\n",
    "print(df)\n",
    "print(\"\\nOne-Hot Encoded:\")\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdcfaa3",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74c30b",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a simple way to represent text data by counting the occurrences of words in a document without considering their order. You can implement the Bag of Words model using Python, specifically using libraries like CountVectorizer from the sklearn.feature_extraction.text module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e29ea40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix:\n",
      "[[0 0 1 0 1 1 0 0 0 0 1]\n",
      " [1 0 1 0 1 0 1 0 1 1 0]\n",
      " [0 1 0 1 1 0 0 1 0 0 0]]\n",
      "\n",
      "Feature Names:\n",
      "['company' 'fear' 'has' 'his' 'increased' 'inflation' 'its' 'pulse' 'sale'\n",
      " 'the' 'unemployment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# List of text documents\n",
    "documents = [\n",
    "    \"Inflation has increased unemployment.\",\n",
    "    \"The company has increased its sale.\",\n",
    "    \"Fear increased his pulse.\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents into a bag-of-words representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the list of feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better readability\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Display the bag-of-words representation\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(X_dense)\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85dbc9",
   "metadata": {},
   "source": [
    "# TF-IDF using SK Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f2a9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.44451431 0.         0.34520502 0.5844829\n",
      "  0.         0.         0.         0.         0.5844829 ]\n",
      " [0.45050407 0.         0.34261996 0.         0.26607496 0.\n",
      "  0.45050407 0.         0.45050407 0.45050407 0.        ]\n",
      " [0.         0.54645401 0.         0.54645401 0.32274454 0.\n",
      "  0.         0.54645401 0.         0.         0.        ]]\n",
      "\n",
      "Feature Names:\n",
      "['company' 'fear' 'has' 'his' 'increased' 'inflation' 'its' 'pulse' 'sale'\n",
      " 'the' 'unemployment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of text documents\n",
    "documents = [\n",
    "    \"Inflation has increased unemployment.\",\n",
    "    \"The company has increased its sale.\",\n",
    "    \"Fear increased his pulse.\"\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into a TF-IDF representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the list of feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better readability\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Display the TF-IDF representation\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X_dense)\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23df0f4",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc6f58",
   "metadata": {},
   "source": [
    "# What is Out Of Vocabulary(OOV) problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafba19e",
   "metadata": {},
   "source": [
    "The Out-Of-Vocabulary (OOV) problem, also known as the Out-Of-Dictionary problem, occurs in natural language processing (NLP) when a word or token that appears in the input text is not present in the vocabulary or dictionary of the language model or system being used.\n",
    "\n",
    "In the context of NLP, a vocabulary or dictionary is a predefined set of words that the language model has been trained on. This vocabulary includes words that the model has learned from its training data. When the model encounters a word that is not part of its vocabulary during inference or processing, it faces the OOV problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
